{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1311864,"sourceType":"datasetVersion","datasetId":759820}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"text-align: center;\">Lab 3</h1>\r\n<h4 style=\"text-align: center;\"><span style=\"text-decoration: underline\">Submitted by :</span> BOULBEN Firdaous</h4>\r\n<h4 style=\"text-align: center;\"><span style=\"text-decoration: underline\">Supervised by\r\n:</span> Pr. EL AACHAK Lotfi</h4>","metadata":{}},{"cell_type":"markdown","source":"## Objective :\nThe main purpose behind this lab is to get familiar with Pytorch, to build deep\r\nneural network architecture for Natural language process by using Sequence Models.","metadata":{}},{"cell_type":"markdown","source":"## Part 1: Classification Task ","metadata":{}},{"cell_type":"markdown","source":"### 1. Data Collection Using Scrapy and BeautifulSoup  ","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T09:57:19.280335Z","iopub.execute_input":"2024-11-26T09:57:19.282701Z","iopub.status.idle":"2024-11-26T09:57:20.537027Z","shell.execute_reply.started":"2024-11-26T09:57:19.282636Z","shell.execute_reply":"2024-11-26T09:57:20.536002Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Scrape Arabic text from websites\ndef scrape_arabic_text(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    paragraphs = soup.find_all('p')\n    texts = [p.text.strip() for p in paragraphs if p.text.strip()]\n    return texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T09:57:37.749026Z","iopub.execute_input":"2024-11-26T09:57:37.749595Z","iopub.status.idle":"2024-11-26T09:57:37.756397Z","shell.execute_reply.started":"2024-11-26T09:57:37.749529Z","shell.execute_reply":"2024-11-26T09:57:37.755316Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Some URLs for Arabic websites\nurls = [\n    \"https://www.hespress.com/\",\n    \"https://www.aljazeera.net/\",\n    \"https://arabic.cnn.com/\",\n    \"https://www.skynewsarabia.com/\",\n    \"https://www.bbc.com/arabic\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:01:16.486666Z","iopub.execute_input":"2024-11-26T10:01:16.487739Z","iopub.status.idle":"2024-11-26T10:01:16.492505Z","shell.execute_reply.started":"2024-11-26T10:01:16.487687Z","shell.execute_reply":"2024-11-26T10:01:16.491338Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Collect data\ntext_data = []\nfor url in urls:\n    texts = scrape_arabic_text(url)\n    for text in texts:\n        score = random.uniform(0, 10)  # Random score between 0 and 10\n        text_data.append({\"Text\": text, \"Score\": score})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:01:30.505429Z","iopub.execute_input":"2024-11-26T10:01:30.506321Z","iopub.status.idle":"2024-11-26T10:01:31.278375Z","shell.execute_reply.started":"2024-11-26T10:01:30.506266Z","shell.execute_reply":"2024-11-26T10:01:31.277343Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Create a dataset\ndf = pd.DataFrame(text_data)\ndf.to_csv(\"arabic_dataset.csv\", index=False)\nprint(\"Data collection completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:01:46.186425Z","iopub.execute_input":"2024-11-26T10:01:46.186852Z","iopub.status.idle":"2024-11-26T10:01:46.198861Z","shell.execute_reply.started":"2024-11-26T10:01:46.186820Z","shell.execute_reply":"2024-11-26T10:01:46.197915Z"}},"outputs":[{"name":"stdout","text":"Data collection completed!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### 2. Preprocessing the Dataset Using NLP","metadata":{}},{"cell_type":"code","source":"# Import NLP libraries\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.isri import ISRIStemmer\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport re\n\nnltk.download('punkt')\nnltk.download('stopwords')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:07:04.570060Z","iopub.execute_input":"2024-11-26T10:07:04.570437Z","iopub.status.idle":"2024-11-26T10:07:04.578135Z","shell.execute_reply.started":"2024-11-26T10:07:04.570403Z","shell.execute_reply":"2024-11-26T10:07:04.577179Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv(\"arabic_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:08:30.738387Z","iopub.execute_input":"2024-11-26T10:08:30.738759Z","iopub.status.idle":"2024-11-26T10:08:30.746004Z","shell.execute_reply.started":"2024-11-26T10:08:30.738726Z","shell.execute_reply":"2024-11-26T10:08:30.745110Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Preprocessing pipeline\ndef preprocess_text(text):\n    # Remove non-Arabic characters\n    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n    # Tokenization\n    tokens = word_tokenize(text)\n    # Remove stop words\n    stop_words = set(stopwords.words('arabic'))\n    tokens = [word for word in tokens if word not in stop_words]\n    # Stemming\n    stemmer = ISRIStemmer()\n    tokens = [stemmer.stem(word) for word in tokens]\n    return \" \".join(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:09:19.140010Z","iopub.execute_input":"2024-11-26T10:09:19.140795Z","iopub.status.idle":"2024-11-26T10:09:19.145868Z","shell.execute_reply.started":"2024-11-26T10:09:19.140763Z","shell.execute_reply":"2024-11-26T10:09:19.144986Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Apply preprocessing\ndf['Processed_Text'] = df['Text'].apply(preprocess_text)\nprint(df.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:09:51.508518Z","iopub.execute_input":"2024-11-26T10:09:51.509055Z","iopub.status.idle":"2024-11-26T10:09:51.556462Z","shell.execute_reply.started":"2024-11-26T10:09:51.509017Z","shell.execute_reply":"2024-11-26T10:09:51.555518Z"}},"outputs":[{"name":"stdout","text":"                                                Text     Score  \\\n0  This website is using a security service to pr...  8.004689   \n1  You can email the site owner to let them know ...  9.043101   \n2  Cloudflare Ray ID: 8e89099e1f76bfd3\\nâ€¢\\n\\n    ...  3.319954   \n3  Ø¹Ù„Ù‰ ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø¬Ø§Ø¹Ø© Ø§Ù„ØªÙŠ ØªÙØ±Ø¶Ù‡Ø§ Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ Ø¹Ù„Ù‰ ØºØ²Ø©Ø› ÙŠ...  2.846044   \n4  Ù…Ø¹ Ø§Ù„ØµÙ…ÙˆØ¯ Ø§Ù„Ø£Ø³Ø·ÙˆØ±ÙŠ Ù„Ø³ÙƒØ§Ù† Ù‚Ø·Ø§Ø¹ ØºØ²Ø© ÙˆØ§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© ÙÙŠ...  8.610400   \n5  ØªÙ…Ø± 75 ÙŠÙˆÙ…Ø§ ÙƒØ§Ù…Ù„Ø© Ø¨ÙŠÙ† Ø¥Ø¹Ù„Ø§Ù† Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª Ø§...  1.229702   \n6  Ù‚Ø§Ù„Øª ØµØ­ÙŠÙØ© Ù„ÙˆÙ…ÙˆÙ†Ø¯ Ø¥Ù† Ø¯Ø®ÙˆÙ„ Ø­Ø±Ø¨ Ø£ÙˆÙƒØ±Ø§Ù†ÙŠØ§ Ù…Ø±Ø­Ù„Ø© Ø¬...  3.615562   \n7  Ù‚Ø§Ù„Øª ØµØ­ÙŠÙØ© Ù†ÙŠÙˆÙŠÙˆØ±Ùƒ ØªØ§ÙŠÙ…Ø² Ø¥Ù† Ø§Ù„Ù…Ù‚ÙŠÙ…ÙŠÙ† Ø¨Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª...  3.913238   \n8  Ø£ÙˆØ¶Ø­ Ù…ÙˆÙ‚Ø¹ Ù…ÙˆÙ†Ø¯ÙˆÙŠØ³ Ø§Ù„Ø£Ù…ÙŠØ±ÙƒÙŠ Ø£Ù† â€œÙ…Ø´Ø±ÙˆØ¹ Ø¥Ø³ØªØ±â€ Ø¨Ø§Øª...  8.785422   \n9  Ù‚Ø§Ù„ ØªÙ‚Ø±ÙŠØ± Ø¨ØµØ­ÙŠÙØ© Ù†ÙŠÙˆÙŠÙˆØ±Ùƒ ØªØ§ÙŠÙ…Ø² Ø¥Ù† Ø¥Ø¯Ø§Ø±Ø© ØªØ±Ø§Ù…Ø¨ ...  4.483984   \n\n                                      Processed_Text  \n0                                                     \n1                                                     \n2                                                     \n3  ÙˆÙ‚Ø¹ Ø¬Ø§Ø¹ ÙØ±Ø¶ Ø±Ø§Ø¦ÙŠÙ„ ØºØ²Ø©Ø› ÙŠØ­Ùƒ Ù‚Ø§Ù„ Ù‚ØµØµ Ø­Ø±Ø¨ ØºØ°ÙŠ Ø´Ù‡Ø¯...  \n4  ØµÙ…Ø¯ Ø³Ø·Ø± Ù„Ø³Ùƒ Ù‚Ø·Ø¹ ØºØ²Ø© Ù‚Ø§Ù… ÙŠÙ‡Ø› ÙØ´Ù„ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡ Ù‚Ø¯Ù… Ø¯...  \n5  ØªÙ…Ø± ÙŠÙˆÙ… ÙƒÙ…Ù„ Ø§Ø¹Ù„ Ù†ØªØ¬ Ù†Ø®Ø¨ Ø±Ø¦Ø³ Ø§Ù…Ø± ÙˆØªÙ„ Ø±Ø¦Ø³ Ø¬Ø¯Ø¯ ØµØ¨...  \n6  Ù‚Ù„Øª ØµØ­Ù ÙˆÙ†Ø¯ Ø¯Ø®Ù„ Ø­Ø±Ø¨ ÙˆÙƒØ± Ø±Ø­Ù„ Ø¬Ø¯Ø¯ ØµØ¹ÙŠØ¯ØŒ Ø¯Ø¹Ø§ Ø¹Ø¯Ø© ...  \n7  Ù‚Ù„Øª ØµØ­Ù ÙˆÙŠØ± ÙŠÙ…Ø² Ù‚ÙŠÙ… ÙˆÙ„ÙŠ ØªØ­Ø¯ ÙˆÙ„Ø¯ Ø®Ø±Ø¬ Ù‡Ø±Ø¹ ØªØµÙ„ ÙƒØª...  \n8  ÙˆØ¶Ø­ ÙˆÙ‚Ø¹ Ù…ÙˆÙ†Ø¯ÙˆÙŠØ³ Ø§Ù…Ø± Ø´Ø±Ø¹ Ø³ØªØ± ÙˆØ¶Ø¹ Ø­Ø¸Ù‰ Ù†Ù‚Ø´ ÙƒØ¨Ø± Ø¯Ø®...  \n9  Ù‚Ø§Ù„ Ù‚Ø±Ø± ØµØ­Ù ÙˆÙŠØ± ÙŠÙ…Ø² Ø¯Ø±Ø© Ø±Ù…Ø¨ Ù‚Ø¯Ù… Ø¸Ù‡Ø± Ø¹ÙƒØ³ ÙˆÙ‚Ø¹ ØªÙ†...  \n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Discretize scores\ndiscretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\ndf['Discrete_Score'] = discretizer.fit_transform(df[['Score']]).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:11:06.688282Z","iopub.execute_input":"2024-11-26T10:11:06.689090Z","iopub.status.idle":"2024-11-26T10:11:06.702395Z","shell.execute_reply.started":"2024-11-26T10:11:06.689055Z","shell.execute_reply":"2024-11-26T10:11:06.701600Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"df.to_csv(\"preprocessed_dataset.csv\", index=False)\nprint(\"Preprocessing completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:11:17.851095Z","iopub.execute_input":"2024-11-26T10:11:17.851448Z","iopub.status.idle":"2024-11-26T10:11:17.859205Z","shell.execute_reply.started":"2024-11-26T10:11:17.851416Z","shell.execute_reply":"2024-11-26T10:11:17.858293Z"}},"outputs":[{"name":"stdout","text":"Preprocessing completed!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"### 3. Train Models (RNN, Bi-RNN, GRU, LSTM)  ","metadata":{}},{"cell_type":"code","source":"# Import PyTorch and supporting libraries\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:12:20.337015Z","iopub.execute_input":"2024-11-26T10:12:20.337384Z","iopub.status.idle":"2024-11-26T10:12:24.566158Z","shell.execute_reply.started":"2024-11-26T10:12:20.337351Z","shell.execute_reply":"2024-11-26T10:12:24.565409Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Load preprocessed dataset\ndf = pd.read_csv(\"preprocessed_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:12:33.937191Z","iopub.execute_input":"2024-11-26T10:12:33.937758Z","iopub.status.idle":"2024-11-26T10:12:33.947248Z","shell.execute_reply.started":"2024-11-26T10:12:33.937708Z","shell.execute_reply":"2024-11-26T10:12:33.946496Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Split data\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(\n    df['Processed_Text'], df['Discrete_Score'], test_size=0.2, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:12:45.699967Z","iopub.execute_input":"2024-11-26T10:12:45.700668Z","iopub.status.idle":"2024-11-26T10:12:45.708022Z","shell.execute_reply.started":"2024-11-26T10:12:45.700633Z","shell.execute_reply":"2024-11-26T10:12:45.707030Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Tokenizer and vocabulary\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:12:53.258685Z","iopub.execute_input":"2024-11-26T10:12:53.259063Z","iopub.status.idle":"2024-11-26T10:12:54.356230Z","shell.execute_reply.started":"2024-11-26T10:12:53.259025Z","shell.execute_reply":"2024-11-26T10:12:54.355078Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d684e32efc44173b3def27ecac968de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f02dc07848d47cab79944a14bbe8b75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b7ece7b7659465aaa9000bec9838dd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"624a202107424818839aa0c5bab95bda"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Dataset class\nclass ArabicDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts.iloc[idx])  # Convert to string\n        label = self.labels.iloc[idx]\n        encoding = self.tokenizer(\n            text, \n            padding='max_length', \n            truncation=True, \n            max_length=self.max_length, \n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:24:56.390507Z","iopub.execute_input":"2024-11-26T10:24:56.390911Z","iopub.status.idle":"2024-11-26T10:24:56.397379Z","shell.execute_reply.started":"2024-11-26T10:24:56.390879Z","shell.execute_reply":"2024-11-26T10:24:56.396209Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Validate dataset\ndf['Text'] = df['Text'].astype(str)  # Ensure all text entries are strings\n\ndf = df.dropna(subset=['Text', 'Score'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:26:26.311017Z","iopub.execute_input":"2024-11-26T10:26:26.311723Z","iopub.status.idle":"2024-11-26T10:26:26.319363Z","shell.execute_reply.started":"2024-11-26T10:26:26.311671Z","shell.execute_reply":"2024-11-26T10:26:26.318269Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Prepare datasets\nmax_length = 128\ntrain_dataset = ArabicDataset(train_texts, train_labels, tokenizer, max_length)\ntest_dataset = ArabicDataset(test_texts, test_labels, tokenizer, max_length)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:27:40.240131Z","iopub.execute_input":"2024-11-26T10:27:40.240935Z","iopub.status.idle":"2024-11-26T10:27:40.245854Z","shell.execute_reply.started":"2024-11-26T10:27:40.240897Z","shell.execute_reply":"2024-11-26T10:27:40.244797Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"#### 3.1. RNN Implementation   ","metadata":{}},{"cell_type":"code","source":"# Define RNN Model\nclass RNNModel(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n        super(RNNModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        _, hidden = self.rnn(x)\n        output = self.fc(hidden.squeeze(0))\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:27:47.048015Z","iopub.execute_input":"2024-11-26T10:27:47.048322Z","iopub.status.idle":"2024-11-26T10:27:47.054487Z","shell.execute_reply.started":"2024-11-26T10:27:47.048296Z","shell.execute_reply":"2024-11-26T10:27:47.053391Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Initialize model, loss, and optimizer\nvocab_size = tokenizer.vocab_size\nembed_size = 128\nhidden_size = 256\noutput_size = len(df['Discrete_Score'].unique())\n\nrnn_model = RNNModel(vocab_size, embed_size, hidden_size, output_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:27:49.639719Z","iopub.execute_input":"2024-11-26T10:27:49.640081Z","iopub.status.idle":"2024-11-26T10:27:49.814583Z","shell.execute_reply.started":"2024-11-26T10:27:49.640047Z","shell.execute_reply":"2024-11-26T10:27:49.813616Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Training loop\ndef train_model(model, train_loader, criterion, optimizer, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            input_ids = batch['input_ids']\n            labels = batch['label']\n            outputs = model(input_ids)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:47:31.917446Z","iopub.execute_input":"2024-11-26T10:47:31.917856Z","iopub.status.idle":"2024-11-26T10:47:31.923898Z","shell.execute_reply.started":"2024-11-26T10:47:31.917822Z","shell.execute_reply":"2024-11-26T10:47:31.922897Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# Train RNN\ntrain_model(rnn_model, train_loader, criterion, optimizer, epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:48:10.957987Z","iopub.execute_input":"2024-11-26T10:48:10.959005Z","iopub.status.idle":"2024-11-26T10:48:26.967689Z","shell.execute_reply.started":"2024-11-26T10:48:10.958956Z","shell.execute_reply":"2024-11-26T10:48:26.966410Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.5665134191513062\nEpoch 2, Loss: 1.5722788572311401\nEpoch 3, Loss: 1.625604510307312\nEpoch 4, Loss: 1.5238451957702637\nEpoch 5, Loss: 1.5186818440755208\nEpoch 6, Loss: 1.5100441376368205\nEpoch 7, Loss: 1.5751574436823528\nEpoch 8, Loss: 1.5095823605855305\nEpoch 9, Loss: 1.5385051568349202\nEpoch 10, Loss: 1.537808895111084\nEpoch 11, Loss: 1.5561861197153728\nEpoch 12, Loss: 1.50415833791097\nEpoch 13, Loss: 1.5343877077102661\nEpoch 14, Loss: 1.5772500038146973\nEpoch 15, Loss: 1.5103242794672649\nEpoch 16, Loss: 1.5225967168807983\nEpoch 17, Loss: 1.5281529029210408\nEpoch 18, Loss: 1.5472896099090576\nEpoch 19, Loss: 1.5260744094848633\nEpoch 20, Loss: 1.5271563132603962\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":"#### 3.2. Bi-RNN Implementation  ","metadata":{}},{"cell_type":"code","source":"# Define Bi-RNN Model\nclass BiRNNModel(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n        super(BiRNNModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_size * 2, output_size)  # Multiply hidden_size by 2 for Bi-directional\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        _, hidden = self.rnn(x)\n        # Combine both directions' hidden states\n        hidden = torch.cat((hidden[0], hidden[1]), dim=1)\n        output = self.fc(hidden)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:29:46.255527Z","iopub.execute_input":"2024-11-26T10:29:46.256239Z","iopub.status.idle":"2024-11-26T10:29:46.262337Z","shell.execute_reply.started":"2024-11-26T10:29:46.256207Z","shell.execute_reply":"2024-11-26T10:29:46.261452Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Initialize the Bi-RNN Model\nbi_rnn_model = BiRNNModel(vocab_size, embed_size, hidden_size, output_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:30:31.862056Z","iopub.execute_input":"2024-11-26T10:30:31.862423Z","iopub.status.idle":"2024-11-26T10:30:32.045106Z","shell.execute_reply.started":"2024-11-26T10:30:31.862390Z","shell.execute_reply":"2024-11-26T10:30:32.044332Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# Train Bi-RNN\ntrain_model(bi_rnn_model, train_loader, criterion, optimizer, epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:48:53.492505Z","iopub.execute_input":"2024-11-26T10:48:53.493254Z","iopub.status.idle":"2024-11-26T10:49:13.522598Z","shell.execute_reply.started":"2024-11-26T10:48:53.493220Z","shell.execute_reply":"2024-11-26T10:49:13.521591Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.671460707982381\nEpoch 2, Loss: 1.6615585486094158\nEpoch 3, Loss: 1.6737428506215413\nEpoch 4, Loss: 1.6803996960322063\nEpoch 5, Loss: 1.6698662439982097\nEpoch 6, Loss: 1.6580512523651123\nEpoch 7, Loss: 1.6821167469024658\nEpoch 8, Loss: 1.669476310412089\nEpoch 9, Loss: 1.6685885588328044\nEpoch 10, Loss: 1.6704107522964478\nEpoch 11, Loss: 1.6645151774088542\nEpoch 12, Loss: 1.680450201034546\nEpoch 13, Loss: 1.6827476024627686\nEpoch 14, Loss: 1.6653744379679363\nEpoch 15, Loss: 1.6629727681477864\nEpoch 16, Loss: 1.6507583061854045\nEpoch 17, Loss: 1.6606574455897014\nEpoch 18, Loss: 1.6654892762502034\nEpoch 19, Loss: 1.6705024639765422\nEpoch 20, Loss: 1.6804960171381633\n","output_type":"stream"}],"execution_count":65},{"cell_type":"markdown","source":"#### 3.3. GRU Implementation  ","metadata":{}},{"cell_type":"code","source":"# Define GRU Model\nclass GRUModel(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n        super(GRUModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        _, hidden = self.gru(x)\n        output = self.fc(hidden.squeeze(0))\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:33:07.025052Z","iopub.execute_input":"2024-11-26T10:33:07.025705Z","iopub.status.idle":"2024-11-26T10:33:07.031543Z","shell.execute_reply.started":"2024-11-26T10:33:07.025670Z","shell.execute_reply":"2024-11-26T10:33:07.030619Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# Initialize the GRU Model\ngru_model = GRUModel(vocab_size, embed_size, hidden_size, output_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:33:33.154871Z","iopub.execute_input":"2024-11-26T10:33:33.155714Z","iopub.status.idle":"2024-11-26T10:33:33.326321Z","shell.execute_reply.started":"2024-11-26T10:33:33.155679Z","shell.execute_reply":"2024-11-26T10:33:33.325289Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Train GRU\ntrain_model(gru_model, train_loader, criterion, optimizer, epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:49:21.732471Z","iopub.execute_input":"2024-11-26T10:49:21.732898Z","iopub.status.idle":"2024-11-26T10:49:30.033321Z","shell.execute_reply.started":"2024-11-26T10:49:21.732863Z","shell.execute_reply":"2024-11-26T10:49:30.032301Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.629641016324361\nEpoch 2, Loss: 1.6444669167200725\nEpoch 3, Loss: 1.6286664406458538\nEpoch 4, Loss: 1.6329840024312336\nEpoch 5, Loss: 1.6425390640894573\nEpoch 6, Loss: 1.6307514905929565\nEpoch 7, Loss: 1.6245061953862507\nEpoch 8, Loss: 1.628666599591573\nEpoch 9, Loss: 1.6372473239898682\nEpoch 10, Loss: 1.6371114651362102\nEpoch 11, Loss: 1.6255133549372356\nEpoch 12, Loss: 1.629776914914449\nEpoch 13, Loss: 1.6360221306482952\nEpoch 14, Loss: 1.636158029238383\nEpoch 15, Loss: 1.6306153933207195\nEpoch 16, Loss: 1.6296409765879314\nEpoch 17, Loss: 1.6288022994995117\nEpoch 18, Loss: 1.6370295683542888\nEpoch 19, Loss: 1.636293927828471\nEpoch 20, Loss: 1.6233958800633748\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"####   3.4. LSTM Implementation","metadata":{}},{"cell_type":"code","source":"# Define LSTM Model\nclass LSTMModel(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n        super(LSTMModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        _, (hidden, _) = self.lstm(x)\n        output = self.fc(hidden.squeeze(0))\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:36:07.783648Z","iopub.execute_input":"2024-11-26T10:36:07.784414Z","iopub.status.idle":"2024-11-26T10:36:07.790167Z","shell.execute_reply.started":"2024-11-26T10:36:07.784378Z","shell.execute_reply":"2024-11-26T10:36:07.789162Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# Initialize the LSTM Model\nlstm_model = LSTMModel(vocab_size, embed_size, hidden_size, output_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:36:10.113842Z","iopub.execute_input":"2024-11-26T10:36:10.114182Z","iopub.status.idle":"2024-11-26T10:36:10.296311Z","shell.execute_reply.started":"2024-11-26T10:36:10.114152Z","shell.execute_reply":"2024-11-26T10:36:10.295613Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# Train LSTM\ntrain_model(lstm_model, train_loader, criterion, optimizer, epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:49:38.177720Z","iopub.execute_input":"2024-11-26T10:49:38.178660Z","iopub.status.idle":"2024-11-26T10:49:43.357624Z","shell.execute_reply.started":"2024-11-26T10:49:38.178624Z","shell.execute_reply":"2024-11-26T10:49:43.356639Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.6123337348302205\nEpoch 2, Loss: 1.6152034997940063\nEpoch 3, Loss: 1.6219777663548787\nEpoch 4, Loss: 1.627034862836202\nEpoch 5, Loss: 1.6129740873972576\nEpoch 6, Loss: 1.6135566631952922\nEpoch 7, Loss: 1.619318167368571\nEpoch 8, Loss: 1.6219136714935303\nEpoch 9, Loss: 1.6142181952794392\nEpoch 10, Loss: 1.6133400599161785\nEpoch 11, Loss: 1.6129741668701172\nEpoch 12, Loss: 1.6241861979166667\nEpoch 13, Loss: 1.6241861581802368\nEpoch 14, Loss: 1.622834841410319\nEpoch 15, Loss: 1.6228347619374592\nEpoch 16, Loss: 1.6164906819661458\nEpoch 17, Loss: 1.6082189877827961\nEpoch 18, Loss: 1.6219136714935303\nEpoch 19, Loss: 1.6168566544850667\nEpoch 20, Loss: 1.6155484914779663\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"### 4. Evaluate Models  ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom nltk.translate.bleu_score import sentence_bleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:38:44.722270Z","iopub.execute_input":"2024-11-26T10:38:44.722654Z","iopub.status.idle":"2024-11-26T10:38:44.727175Z","shell.execute_reply.started":"2024-11-26T10:38:44.722620Z","shell.execute_reply":"2024-11-26T10:38:44.726232Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"def evaluate_model(model, data_loader, criterion):\n    model.eval()\n    total_loss = 0\n    predictions = []\n    ground_truth = []\n    bleu_scores = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids']\n            labels = batch['label']\n\n            outputs = model(input_ids)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n\n            # Save predictions and labels for metrics\n            preds = torch.argmax(outputs, dim=1).tolist()\n            predictions.extend(preds)\n            ground_truth.extend(labels.tolist())\n\n            # BLEU: For demonstration, treating predictions as sequences of single tokens\n            for pred, label in zip(preds, labels.tolist()):\n                bleu_scores.append(sentence_bleu([[str(label)]], [str(pred)]))  # Modify if predictions are sequences\n\n    # Standard Metrics\n    accuracy = accuracy_score(ground_truth, predictions)\n    precision = precision_score(ground_truth, predictions, average='weighted')\n    recall = recall_score(ground_truth, predictions, average='weighted')\n    f1 = f1_score(ground_truth, predictions, average='weighted')\n\n    # BLEU Score\n    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n\n    results = {\n        'loss': total_loss / len(data_loader),\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'bleu_score': avg_bleu\n    }\n    \n    # Display results row by row\n    for metric, value in results.items():\n        print(f\"{metric.capitalize()}: {value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:41:14.999912Z","iopub.execute_input":"2024-11-26T10:41:15.000283Z","iopub.status.idle":"2024-11-26T10:41:15.009497Z","shell.execute_reply.started":"2024-11-26T10:41:15.000254Z","shell.execute_reply":"2024-11-26T10:41:15.008626Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"print(\"RNN Metrics:\\n\")\nrnn_metrics = evaluate_model(rnn_model, test_loader, criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:50:05.932272Z","iopub.execute_input":"2024-11-26T10:50:05.932618Z","iopub.status.idle":"2024-11-26T10:50:05.969992Z","shell.execute_reply.started":"2024-11-26T10:50:05.932582Z","shell.execute_reply":"2024-11-26T10:50:05.968871Z"}},"outputs":[{"name":"stdout","text":"RNN Metrics:\n\nLoss: 1.6429\nAccuracy: 0.2727\nPrecision: 0.0744\nRecall: 0.2727\nF1_score: 0.1169\nBleu_score: 0.2727\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"print(\"Bi-RNN Metrics:\\n\")\nbi_rnn_metrics = evaluate_model(bi_rnn_model, test_loader, criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:50:23.728059Z","iopub.execute_input":"2024-11-26T10:50:23.728392Z","iopub.status.idle":"2024-11-26T10:50:23.772867Z","shell.execute_reply.started":"2024-11-26T10:50:23.728362Z","shell.execute_reply":"2024-11-26T10:50:23.771780Z"}},"outputs":[{"name":"stdout","text":"Bi-RNN Metrics:\n\nLoss: 1.6324\nAccuracy: 0.0909\nPrecision: 0.0101\nRecall: 0.0909\nF1_score: 0.0182\nBleu_score: 0.0909\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"print(\"GRU Metrics:\\n\")\ngru_metrics = evaluate_model(gru_model, test_loader, criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:50:33.072637Z","iopub.execute_input":"2024-11-26T10:50:33.073326Z","iopub.status.idle":"2024-11-26T10:50:33.120425Z","shell.execute_reply.started":"2024-11-26T10:50:33.073297Z","shell.execute_reply":"2024-11-26T10:50:33.119383Z"}},"outputs":[{"name":"stdout","text":"GRU Metrics:\n\nLoss: 1.6078\nAccuracy: 0.2727\nPrecision: 0.0744\nRecall: 0.2727\nF1_score: 0.1169\nBleu_score: 0.2727\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"print(\"LSTM Metrics:\\n\")\nlstm_metrics = evaluate_model(lstm_model, test_loader, criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:50:37.826143Z","iopub.execute_input":"2024-11-26T10:50:37.826723Z","iopub.status.idle":"2024-11-26T10:50:37.853204Z","shell.execute_reply.started":"2024-11-26T10:50:37.826691Z","shell.execute_reply":"2024-11-26T10:50:37.852172Z"}},"outputs":[{"name":"stdout","text":"LSTM Metrics:\n\nLoss: 1.6178\nAccuracy: 0.2727\nPrecision: 0.0744\nRecall: 0.2727\nF1_score: 0.1169\nBleu_score: 0.2727\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":71},{"cell_type":"markdown","source":"## Part 2: Transformer (Text generation) ","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom transformers import TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:00:59.185038Z","iopub.execute_input":"2024-11-26T11:00:59.185707Z","iopub.status.idle":"2024-11-26T11:01:11.770511Z","shell.execute_reply.started":"2024-11-26T11:00:59.185669Z","shell.execute_reply":"2024-11-26T11:01:11.769538Z"}},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":"### 1. Fine tune the pre-trained model (GPT2)  ","metadata":{}},{"cell_type":"markdown","source":"#### 1.1. Load the Pre-trained GPT-2 Model   ","metadata":{}},{"cell_type":"code","source":"# Load pre-trained GPT-2 model and tokenizer\nmodel_name = \"gpt2\"\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\n\n# Set the model in evaluation mode\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:02:43.295885Z","iopub.execute_input":"2024-11-26T11:02:43.296716Z","iopub.status.idle":"2024-11-26T11:02:50.689079Z","shell.execute_reply.started":"2024-11-26T11:02:43.296679Z","shell.execute_reply":"2024-11-26T11:02:50.688093Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd4a963991ea4eaeb5da35878d20e5e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1405071bffc49ceb72a0919e4d7819a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b81dea9a84e4df8a71ab08dbf5953cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02de4767e59c4aa4bfee1068a0e66754"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11e0b5efe7914ae4a8ee814458a99acc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cf49dbc248d44ac990c75a70f69336a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e76a840ead9749a58310df8b28f1dffa"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":73},{"cell_type":"markdown","source":"#### 1.2. Load Shakespeare Dataset   ","metadata":{}},{"cell_type":"code","source":"# Load the dataset\ndataset_path = \"/kaggle/input/shakespeare-text/text.txt\"\n\n# Read the text dataset\nwith open(dataset_path, 'r') as file:\n    shakespeare_text = file.read()\n\n# Check the first 500 characters\nprint(shakespeare_text[:500])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:05:32.901487Z","iopub.execute_input":"2024-11-26T11:05:32.901953Z","iopub.status.idle":"2024-11-26T11:05:32.927917Z","shell.execute_reply.started":"2024-11-26T11:05:32.901918Z","shell.execute_reply":"2024-11-26T11:05:32.927067Z"}},"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor\n","output_type":"stream"}],"execution_count":74},{"cell_type":"markdown","source":"#### 1.3. Fine-tuning GPT-2 on the Shakespeare Dataset   ","metadata":{}},{"cell_type":"code","source":"# Prepare the dataset for fine-tuning\ntrain_file = \"/kaggle/working/shakespeare_train.txt\"\n\n# Write the dataset to a temporary file (required by TextDataset)\nwith open(train_file, \"w\") as file:\n    file.write(shakespeare_text)\n\n# Load the dataset using TextDataset\ntrain_dataset = TextDataset(\n    tokenizer=tokenizer,\n    file_path=train_file,\n    block_size=128  # Sequence length\n)\n\n# Set up data collator (used for padding and batching)\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # GPT-2 uses causal language modeling\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:11:14.882263Z","iopub.execute_input":"2024-11-26T11:11:14.883105Z","iopub.status.idle":"2024-11-26T11:11:14.908327Z","shell.execute_reply.started":"2024-11-26T11:11:14.883069Z","shell.execute_reply":"2024-11-26T11:11:14.907421Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./gpt2_finetuned\",  # Where to save the fine-tuned model\n    overwrite_output_dir=True,\n    num_train_epochs=5,  # Train for 3 epochs (adjustable)\n    per_device_train_batch_size=4,\n    save_steps=500,  # Save model every 500 steps\n    logging_steps=100,  # Log every 100 steps\n    report_to=[\"none\"]\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset\n)\n\n# Fine-tune the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:11:39.824801Z","iopub.execute_input":"2024-11-26T11:11:39.825156Z","iopub.status.idle":"2024-11-26T11:20:00.235996Z","shell.execute_reply.started":"2024-11-26T11:11:39.825126Z","shell.execute_reply":"2024-11-26T11:20:00.235192Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1650' max='1650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1650/1650 08:18, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>3.881300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>3.658600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>3.624300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>3.490700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>3.398900</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>3.414400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>3.357300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>3.269600</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>3.283300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.271900</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>3.199800</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>3.193600</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>3.209900</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>3.143900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>3.133800</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>3.153600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1650, training_loss=3.34943359375, metrics={'train_runtime': 499.9363, 'train_samples_per_second': 26.403, 'train_steps_per_second': 3.3, 'total_flos': 862263705600000.0, 'train_loss': 3.34943359375, 'epoch': 5.0})"},"metadata":{}}],"execution_count":77},{"cell_type":"markdown","source":"### 2. Generate Text Using the Fine-tuned Model  ","metadata":{}},{"cell_type":"code","source":"# Ensure the model and inputs are on the correct device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:24:25.758606Z","iopub.execute_input":"2024-11-26T11:24:25.759486Z","iopub.status.idle":"2024-11-26T11:24:25.770697Z","shell.execute_reply.started":"2024-11-26T11:24:25.759452Z","shell.execute_reply":"2024-11-26T11:24:25.769830Z"}},"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":81},{"cell_type":"code","source":"# Define an input sentence\ninput_sentence = \"As the sun set, the world seemed to quiet down\"\n\n# Encode the input sentence\ninput_ids = tokenizer.encode(input_sentence, return_tensors='pt').to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:31:40.453005Z","iopub.execute_input":"2024-11-26T11:31:40.453857Z","iopub.status.idle":"2024-11-26T11:31:40.458719Z","shell.execute_reply.started":"2024-11-26T11:31:40.453821Z","shell.execute_reply":"2024-11-26T11:31:40.457788Z"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":"# Generate new text (next tokens) using the fine-tuned model\ngenerated_ids = model.generate(\n    input_ids=input_ids,\n    max_length=200,  # Length of the generated text\n    num_return_sequences=1,  # Generate one sequence\n    temperature=0.9,  # Add randomness\n    top_k=50,  # Limit to the top 50 most likely next words\n    top_p=0.95,  # Use top-p sampling for diversity\n    no_repeat_ngram_size=2,  # Prevent repeating phrases\n    pad_token_id=tokenizer.eos_token_id  # Avoid issues with padding\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:31:42.057814Z","iopub.execute_input":"2024-11-26T11:31:42.058192Z","iopub.status.idle":"2024-11-26T11:31:43.948624Z","shell.execute_reply.started":"2024-11-26T11:31:42.058159Z","shell.execute_reply":"2024-11-26T11:31:43.947599Z"}},"outputs":[],"execution_count":103},{"cell_type":"code","source":"# Decode the generated ids to text\ngenerated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\n# Print the generated text\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:31:45.547258Z","iopub.execute_input":"2024-11-26T11:31:45.548055Z","iopub.status.idle":"2024-11-26T11:31:45.556118Z","shell.execute_reply.started":"2024-11-26T11:31:45.548019Z","shell.execute_reply":"2024-11-26T11:31:45.555069Z"}},"outputs":[{"name":"stdout","text":"As the sun set, the world seemed to quiet down,\nAnd the stars were still in their stars.\n\nBENVOLIO:\nO, I am a little too late!\nI am not a man to be late. I have been\nA little late, and am yet a very late;\nBut I will be so, for I must be a late\nTo be the late of the day. Come, come, my lord. What\nYou have done, you have made a mistake. You\nHave made an error, sir, in your haste. Your\nson, Angelo, is dead; and you, your son, are dead. Go, go, good sir; go. Away, away, home, Away! Away. Wherefore, what is your\ngoodly son?\nWhat is his name? Angelo? What is he? I'll tell you. Angelo! what\nis he, that is not Angelo: he is a poor,\n","output_type":"stream"}],"execution_count":104},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}