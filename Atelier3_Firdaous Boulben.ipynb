{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1311864,"sourceType":"datasetVersion","datasetId":759820}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"text-align: center;\">Lab 3</h1>\r\n<h4 style=\"text-align: center;\"><span style=\"text-decoration: underline\">Submitted by :</span> BOULBEN Firdaous</h4>\r\n<h4 style=\"text-align: center;\"><span style=\"text-decoration: underline\">Supervised by\r\n:</span> Pr. EL AACHAK Lotfi</h4>","metadata":{}},{"cell_type":"markdown","source":"## Objective :\nThe main purpose behind this lab is to get familiar with Pytorch, to build deep\r\nneural network architecture for Natural language process by using Sequence Models.","metadata":{}},{"cell_type":"markdown","source":"## Part 1: Classification Task ","metadata":{}},{"cell_type":"markdown","source":"### 1. Data Collection Using Scrapy and BeautifulSoup  ","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T09:57:19.280335Z","iopub.execute_input":"2024-11-26T09:57:19.282701Z","iopub.status.idle":"2024-11-26T09:57:20.537027Z","shell.execute_reply.started":"2024-11-26T09:57:19.282636Z","shell.execute_reply":"2024-11-26T09:57:20.536002Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Scrape Arabic text from websites\ndef scrape_arabic_text(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    paragraphs = soup.find_all('p')\n    texts = [p.text.strip() for p in paragraphs if p.text.strip()]\n    return texts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T09:57:37.749026Z","iopub.execute_input":"2024-11-26T09:57:37.749595Z","iopub.status.idle":"2024-11-26T09:57:37.756397Z","shell.execute_reply.started":"2024-11-26T09:57:37.749529Z","shell.execute_reply":"2024-11-26T09:57:37.755316Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Some URLs for Arabic websites\nurls = [\n    \"https://www.hespress.com/\",\n    \"https://www.aljazeera.net/\",\n    \"https://arabic.cnn.com/\",\n    \"https://www.skynewsarabia.com/\",\n    \"https://www.bbc.com/arabic\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:01:16.486666Z","iopub.execute_input":"2024-11-26T10:01:16.487739Z","iopub.status.idle":"2024-11-26T10:01:16.492505Z","shell.execute_reply.started":"2024-11-26T10:01:16.487687Z","shell.execute_reply":"2024-11-26T10:01:16.491338Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Collect data\ntext_data = []\nfor url in urls:\n    texts = scrape_arabic_text(url)\n    for text in texts:\n        score = random.uniform(0, 10)  # Random score between 0 and 10\n        text_data.append({\"Text\": text, \"Score\": score})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:01:30.505429Z","iopub.execute_input":"2024-11-26T10:01:30.506321Z","iopub.status.idle":"2024-11-26T10:01:31.278375Z","shell.execute_reply.started":"2024-11-26T10:01:30.506266Z","shell.execute_reply":"2024-11-26T10:01:31.277343Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Create a dataset\ndf = pd.DataFrame(text_data)\ndf.to_csv(\"arabic_dataset.csv\", index=False)\nprint(\"Data collection completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:01:46.186425Z","iopub.execute_input":"2024-11-26T10:01:46.186852Z","iopub.status.idle":"2024-11-26T10:01:46.198861Z","shell.execute_reply.started":"2024-11-26T10:01:46.186820Z","shell.execute_reply":"2024-11-26T10:01:46.197915Z"}},"outputs":[{"name":"stdout","text":"Data collection completed!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### 2. Preprocessing the Dataset Using NLP","metadata":{}},{"cell_type":"code","source":"# Import NLP libraries\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.isri import ISRIStemmer\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport re\n\nnltk.download('punkt')\nnltk.download('stopwords')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:07:04.570060Z","iopub.execute_input":"2024-11-26T10:07:04.570437Z","iopub.status.idle":"2024-11-26T10:07:04.578135Z","shell.execute_reply.started":"2024-11-26T10:07:04.570403Z","shell.execute_reply":"2024-11-26T10:07:04.577179Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv(\"arabic_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:08:30.738387Z","iopub.execute_input":"2024-11-26T10:08:30.738759Z","iopub.status.idle":"2024-11-26T10:08:30.746004Z","shell.execute_reply.started":"2024-11-26T10:08:30.738726Z","shell.execute_reply":"2024-11-26T10:08:30.745110Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Preprocessing pipeline\ndef preprocess_text(text):\n    # Remove non-Arabic characters\n    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n    # Tokenization\n    tokens = word_tokenize(text)\n    # Remove stop words\n    stop_words = set(stopwords.words('arabic'))\n    tokens = [word for word in tokens if word not in stop_words]\n    # Stemming\n    stemmer = ISRIStemmer()\n    tokens = [stemmer.stem(word) for word in tokens]\n    return \" \".join(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:09:19.140010Z","iopub.execute_input":"2024-11-26T10:09:19.140795Z","iopub.status.idle":"2024-11-26T10:09:19.145868Z","shell.execute_reply.started":"2024-11-26T10:09:19.140763Z","shell.execute_reply":"2024-11-26T10:09:19.144986Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Apply preprocessing\ndf['Processed_Text'] = df['Text'].apply(preprocess_text)\nprint(df.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:09:51.508518Z","iopub.execute_input":"2024-11-26T10:09:51.509055Z","iopub.status.idle":"2024-11-26T10:09:51.556462Z","shell.execute_reply.started":"2024-11-26T10:09:51.509017Z","shell.execute_reply":"2024-11-26T10:09:51.555518Z"}},"outputs":[{"name":"stdout","text":"                                                Text     Score  \\\n0  This website is using a security service to pr...  8.004689   \n1  You can email the site owner to let them know ...  9.043101   \n2  Cloudflare Ray ID: 8e89099e1f76bfd3\\n•\\n\\n    ...  3.319954   \n3  على وقع المجاعة التي تفرضها إسرائيل على غزة؛ ي...  2.846044   \n4  مع الصمود الأسطوري لسكان قطاع غزة والمقاومة في...  8.610400   \n5  تمر 75 يوما كاملة بين إعلان نتيجة الانتخابات ا...  1.229702   \n6  قالت صحيفة لوموند إن دخول حرب أوكرانيا مرحلة ج...  3.615562   \n7  قالت صحيفة نيويورك تايمز إن المقيمين بالولايات...  3.913238   \n8  أوضح موقع موندويس الأميركي أن “مشروع إستر” بات...  8.785422   \n9  قال تقرير بصحيفة نيويورك تايمز إن إدارة ترامب ...  4.483984   \n\n                                      Processed_Text  \n0                                                     \n1                                                     \n2                                                     \n3  وقع جاع فرض رائيل غزة؛ يحك قال قصص حرب غذي شهد...  \n4  صمد سطر لسك قطع غزة قام يه؛ فشل سيناريوه قدم د...  \n5  تمر يوم كمل اعل نتج نخب رئس امر وتل رئس جدد صب...  \n6  قلت صحف وند دخل حرب وكر رحل جدد صعيد، دعا عدة ...  \n7  قلت صحف وير يمز قيم ولي تحد ولد خرج هرع تصل كت...  \n8  وضح وقع موندويس امر شرع ستر وضع حظى نقش كبر دخ...  \n9  قال قرر صحف وير يمز درة رمب قدم ظهر عكس وقع تن...  \n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Discretize scores\ndiscretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\ndf['Discrete_Score'] = discretizer.fit_transform(df[['Score']]).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:11:06.688282Z","iopub.execute_input":"2024-11-26T10:11:06.689090Z","iopub.status.idle":"2024-11-26T10:11:06.702395Z","shell.execute_reply.started":"2024-11-26T10:11:06.689055Z","shell.execute_reply":"2024-11-26T10:11:06.701600Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"df.to_csv(\"preprocessed_dataset.csv\", index=False)\nprint(\"Preprocessing completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:11:17.851095Z","iopub.execute_input":"2024-11-26T10:11:17.851448Z","iopub.status.idle":"2024-11-26T10:11:17.859205Z","shell.execute_reply.started":"2024-11-26T10:11:17.851416Z","shell.execute_reply":"2024-11-26T10:11:17.858293Z"}},"outputs":[{"name":"stdout","text":"Preprocessing completed!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"### 3. Train Models (RNN, Bi-RNN, GRU, LSTM)  ","metadata":{}},{"cell_type":"code","source":"# Import PyTorch and supporting libraries\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:12:20.337015Z","iopub.execute_input":"2024-11-26T10:12:20.337384Z","iopub.status.idle":"2024-11-26T10:12:24.566158Z","shell.execute_reply.started":"2024-11-26T10:12:20.337351Z","shell.execute_reply":"2024-11-26T10:12:24.565409Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Load preprocessed dataset\ndf = pd.read_csv(\"preprocessed_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:12:33.937191Z","iopub.execute_input":"2024-11-26T10:12:33.937758Z","iopub.status.idle":"2024-11-26T10:12:33.947248Z","shell.execute_reply.started":"2024-11-26T10:12:33.937708Z","shell.execute_reply":"2024-11-26T10:12:33.946496Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Split data\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(\n    df['Processed_Text'], df['Discrete_Score'], test_size=0.2, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:12:45.699967Z","iopub.execute_input":"2024-11-26T10:12:45.700668Z","iopub.status.idle":"2024-11-26T10:12:45.708022Z","shell.execute_reply.started":"2024-11-26T10:12:45.700633Z","shell.execute_reply":"2024-11-26T10:12:45.707030Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Tokenizer and vocabulary\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:12:53.258685Z","iopub.execute_input":"2024-11-26T10:12:53.259063Z","iopub.status.idle":"2024-11-26T10:12:54.356230Z","shell.execute_reply.started":"2024-11-26T10:12:53.259025Z","shell.execute_reply":"2024-11-26T10:12:54.355078Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d684e32efc44173b3def27ecac968de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f02dc07848d47cab79944a14bbe8b75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b7ece7b7659465aaa9000bec9838dd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"624a202107424818839aa0c5bab95bda"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Dataset class\nclass ArabicDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts.iloc[idx])  # Convert to string\n        label = self.labels.iloc[idx]\n        encoding = self.tokenizer(\n            text, \n            padding='max_length', \n            truncation=True, \n            max_length=self.max_length, \n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:24:56.390507Z","iopub.execute_input":"2024-11-26T10:24:56.390911Z","iopub.status.idle":"2024-11-26T10:24:56.397379Z","shell.execute_reply.started":"2024-11-26T10:24:56.390879Z","shell.execute_reply":"2024-11-26T10:24:56.396209Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Validate dataset\ndf['Text'] = df['Text'].astype(str)  # Ensure all text entries are strings\n\ndf = df.dropna(subset=['Text', 'Score'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:26:26.311017Z","iopub.execute_input":"2024-11-26T10:26:26.311723Z","iopub.status.idle":"2024-11-26T10:26:26.319363Z","shell.execute_reply.started":"2024-11-26T10:26:26.311671Z","shell.execute_reply":"2024-11-26T10:26:26.318269Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Prepare datasets\nmax_length = 128\ntrain_dataset = ArabicDataset(train_texts, train_labels, tokenizer, max_length)\ntest_dataset = ArabicDataset(test_texts, test_labels, tokenizer, max_length)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:27:40.240131Z","iopub.execute_input":"2024-11-26T10:27:40.240935Z","iopub.status.idle":"2024-11-26T10:27:40.245854Z","shell.execute_reply.started":"2024-11-26T10:27:40.240897Z","shell.execute_reply":"2024-11-26T10:27:40.244797Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"#### 3.1. RNN Implementation   ","metadata":{}},{"cell_type":"code","source":"# Define RNN Model\nclass RNNModel(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n        super(RNNModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        _, hidden = self.rnn(x)\n        output = self.fc(hidden.squeeze(0))\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:27:47.048015Z","iopub.execute_input":"2024-11-26T10:27:47.048322Z","iopub.status.idle":"2024-11-26T10:27:47.054487Z","shell.execute_reply.started":"2024-11-26T10:27:47.048296Z","shell.execute_reply":"2024-11-26T10:27:47.053391Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Initialize model, loss, and optimizer\nvocab_size = tokenizer.vocab_size\nembed_size = 128\nhidden_size = 256\noutput_size = len(df['Discrete_Score'].unique())\n\nrnn_model = RNNModel(vocab_size, embed_size, hidden_size, output_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn_model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:27:49.639719Z","iopub.execute_input":"2024-11-26T10:27:49.640081Z","iopub.status.idle":"2024-11-26T10:27:49.814583Z","shell.execute_reply.started":"2024-11-26T10:27:49.640047Z","shell.execute_reply":"2024-11-26T10:27:49.813616Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Training loop\ndef train_model(model, train_loader, criterion, optimizer, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            input_ids = batch['input_ids']\n            labels = batch['label']\n            outputs = model(input_ids)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:47:31.917446Z","iopub.execute_input":"2024-11-26T10:47:31.917856Z","iopub.status.idle":"2024-11-26T10:47:31.923898Z","shell.execute_reply.started":"2024-11-26T10:47:31.917822Z","shell.execute_reply":"2024-11-26T10:47:31.922897Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# Train RNN\ntrain_model(rnn_model, train_loader, criterion, optimizer, epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:48:10.957987Z","iopub.execute_input":"2024-11-26T10:48:10.959005Z","iopub.status.idle":"2024-11-26T10:48:26.967689Z","shell.execute_reply.started":"2024-11-26T10:48:10.958956Z","shell.execute_reply":"2024-11-26T10:48:26.966410Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.5665134191513062\nEpoch 2, Loss: 1.5722788572311401\nEpoch 3, Loss: 1.625604510307312\nEpoch 4, Loss: 1.5238451957702637\nEpoch 5, Loss: 1.5186818440755208\nEpoch 6, Loss: 1.5100441376368205\nEpoch 7, Loss: 1.5751574436823528\nEpoch 8, Loss: 1.5095823605855305\nEpoch 9, Loss: 1.5385051568349202\nEpoch 10, Loss: 1.537808895111084\nEpoch 11, Loss: 1.5561861197153728\nEpoch 12, Loss: 1.50415833791097\nEpoch 13, Loss: 1.5343877077102661\nEpoch 14, Loss: 1.5772500038146973\nEpoch 15, Loss: 1.5103242794672649\nEpoch 16, Loss: 1.5225967168807983\nEpoch 17, Loss: 1.5281529029210408\nEpoch 18, Loss: 1.5472896099090576\nEpoch 19, Loss: 1.5260744094848633\nEpoch 20, Loss: 1.5271563132603962\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":"#### 3.2. Bi-RNN Implementation  ","metadata":{}},{"cell_type":"code","source":"# Define Bi-RNN Model\nclass BiRNNModel(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n        super(BiRNNModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_size * 2, output_size)  # Multiply hidden_size by 2 for Bi-directional\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        _, hidden = self.rnn(x)\n        # Combine both directions' hidden states\n        hidden = torch.cat((hidden[0], hidden[1]), dim=1)\n        output = self.fc(hidden)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:29:46.255527Z","iopub.execute_input":"2024-11-26T10:29:46.256239Z","iopub.status.idle":"2024-11-26T10:29:46.262337Z","shell.execute_reply.started":"2024-11-26T10:29:46.256207Z","shell.execute_reply":"2024-11-26T10:29:46.261452Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Initialize the Bi-RNN Model\nbi_rnn_model = BiRNNModel(vocab_size, embed_size, hidden_size, output_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:30:31.862056Z","iopub.execute_input":"2024-11-26T10:30:31.862423Z","iopub.status.idle":"2024-11-26T10:30:32.045106Z","shell.execute_reply.started":"2024-11-26T10:30:31.862390Z","shell.execute_reply":"2024-11-26T10:30:32.044332Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# Train Bi-RNN\ntrain_model(bi_rnn_model, train_loader, criterion, optimizer, epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:48:53.492505Z","iopub.execute_input":"2024-11-26T10:48:53.493254Z","iopub.status.idle":"2024-11-26T10:49:13.522598Z","shell.execute_reply.started":"2024-11-26T10:48:53.493220Z","shell.execute_reply":"2024-11-26T10:49:13.521591Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.671460707982381\nEpoch 2, Loss: 1.6615585486094158\nEpoch 3, Loss: 1.6737428506215413\nEpoch 4, Loss: 1.6803996960322063\nEpoch 5, Loss: 1.6698662439982097\nEpoch 6, Loss: 1.6580512523651123\nEpoch 7, Loss: 1.6821167469024658\nEpoch 8, Loss: 1.669476310412089\nEpoch 9, Loss: 1.6685885588328044\nEpoch 10, Loss: 1.6704107522964478\nEpoch 11, Loss: 1.6645151774088542\nEpoch 12, Loss: 1.680450201034546\nEpoch 13, Loss: 1.6827476024627686\nEpoch 14, Loss: 1.6653744379679363\nEpoch 15, Loss: 1.6629727681477864\nEpoch 16, Loss: 1.6507583061854045\nEpoch 17, Loss: 1.6606574455897014\nEpoch 18, Loss: 1.6654892762502034\nEpoch 19, Loss: 1.6705024639765422\nEpoch 20, Loss: 1.6804960171381633\n","output_type":"stream"}],"execution_count":65},{"cell_type":"markdown","source":"#### 3.3. GRU Implementation  ","metadata":{}},{"cell_type":"code","source":"# Define GRU Model\nclass GRUModel(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n        super(GRUModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        _, hidden = self.gru(x)\n        output = self.fc(hidden.squeeze(0))\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:33:07.025052Z","iopub.execute_input":"2024-11-26T10:33:07.025705Z","iopub.status.idle":"2024-11-26T10:33:07.031543Z","shell.execute_reply.started":"2024-11-26T10:33:07.025670Z","shell.execute_reply":"2024-11-26T10:33:07.030619Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# Initialize the GRU Model\ngru_model = GRUModel(vocab_size, embed_size, hidden_size, output_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:33:33.154871Z","iopub.execute_input":"2024-11-26T10:33:33.155714Z","iopub.status.idle":"2024-11-26T10:33:33.326321Z","shell.execute_reply.started":"2024-11-26T10:33:33.155679Z","shell.execute_reply":"2024-11-26T10:33:33.325289Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Train GRU\ntrain_model(gru_model, train_loader, criterion, optimizer, epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:49:21.732471Z","iopub.execute_input":"2024-11-26T10:49:21.732898Z","iopub.status.idle":"2024-11-26T10:49:30.033321Z","shell.execute_reply.started":"2024-11-26T10:49:21.732863Z","shell.execute_reply":"2024-11-26T10:49:30.032301Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.629641016324361\nEpoch 2, Loss: 1.6444669167200725\nEpoch 3, Loss: 1.6286664406458538\nEpoch 4, Loss: 1.6329840024312336\nEpoch 5, Loss: 1.6425390640894573\nEpoch 6, Loss: 1.6307514905929565\nEpoch 7, Loss: 1.6245061953862507\nEpoch 8, Loss: 1.628666599591573\nEpoch 9, Loss: 1.6372473239898682\nEpoch 10, Loss: 1.6371114651362102\nEpoch 11, Loss: 1.6255133549372356\nEpoch 12, Loss: 1.629776914914449\nEpoch 13, Loss: 1.6360221306482952\nEpoch 14, Loss: 1.636158029238383\nEpoch 15, Loss: 1.6306153933207195\nEpoch 16, Loss: 1.6296409765879314\nEpoch 17, Loss: 1.6288022994995117\nEpoch 18, Loss: 1.6370295683542888\nEpoch 19, Loss: 1.636293927828471\nEpoch 20, Loss: 1.6233958800633748\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"####   3.4. LSTM Implementation","metadata":{}},{"cell_type":"code","source":"# Define LSTM Model\nclass LSTMModel(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n        super(LSTMModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_ids):\n        x = self.embedding(input_ids)\n        _, (hidden, _) = self.lstm(x)\n        output = self.fc(hidden.squeeze(0))\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:36:07.783648Z","iopub.execute_input":"2024-11-26T10:36:07.784414Z","iopub.status.idle":"2024-11-26T10:36:07.790167Z","shell.execute_reply.started":"2024-11-26T10:36:07.784378Z","shell.execute_reply":"2024-11-26T10:36:07.789162Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# Initialize the LSTM Model\nlstm_model = LSTMModel(vocab_size, embed_size, hidden_size, output_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:36:10.113842Z","iopub.execute_input":"2024-11-26T10:36:10.114182Z","iopub.status.idle":"2024-11-26T10:36:10.296311Z","shell.execute_reply.started":"2024-11-26T10:36:10.114152Z","shell.execute_reply":"2024-11-26T10:36:10.295613Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# Train LSTM\ntrain_model(lstm_model, train_loader, criterion, optimizer, epochs=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:49:38.177720Z","iopub.execute_input":"2024-11-26T10:49:38.178660Z","iopub.status.idle":"2024-11-26T10:49:43.357624Z","shell.execute_reply.started":"2024-11-26T10:49:38.178624Z","shell.execute_reply":"2024-11-26T10:49:43.356639Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.6123337348302205\nEpoch 2, Loss: 1.6152034997940063\nEpoch 3, Loss: 1.6219777663548787\nEpoch 4, Loss: 1.627034862836202\nEpoch 5, Loss: 1.6129740873972576\nEpoch 6, Loss: 1.6135566631952922\nEpoch 7, Loss: 1.619318167368571\nEpoch 8, Loss: 1.6219136714935303\nEpoch 9, Loss: 1.6142181952794392\nEpoch 10, Loss: 1.6133400599161785\nEpoch 11, Loss: 1.6129741668701172\nEpoch 12, Loss: 1.6241861979166667\nEpoch 13, Loss: 1.6241861581802368\nEpoch 14, Loss: 1.622834841410319\nEpoch 15, Loss: 1.6228347619374592\nEpoch 16, Loss: 1.6164906819661458\nEpoch 17, Loss: 1.6082189877827961\nEpoch 18, Loss: 1.6219136714935303\nEpoch 19, Loss: 1.6168566544850667\nEpoch 20, Loss: 1.6155484914779663\n","output_type":"stream"}],"execution_count":67},{"cell_type":"markdown","source":"### 4. Evaluate Models  ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom nltk.translate.bleu_score import sentence_bleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:38:44.722270Z","iopub.execute_input":"2024-11-26T10:38:44.722654Z","iopub.status.idle":"2024-11-26T10:38:44.727175Z","shell.execute_reply.started":"2024-11-26T10:38:44.722620Z","shell.execute_reply":"2024-11-26T10:38:44.726232Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"def evaluate_model(model, data_loader, criterion):\n    model.eval()\n    total_loss = 0\n    predictions = []\n    ground_truth = []\n    bleu_scores = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids']\n            labels = batch['label']\n\n            outputs = model(input_ids)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n\n            # Save predictions and labels for metrics\n            preds = torch.argmax(outputs, dim=1).tolist()\n            predictions.extend(preds)\n            ground_truth.extend(labels.tolist())\n\n            # BLEU: For demonstration, treating predictions as sequences of single tokens\n            for pred, label in zip(preds, labels.tolist()):\n                bleu_scores.append(sentence_bleu([[str(label)]], [str(pred)]))  # Modify if predictions are sequences\n\n    # Standard Metrics\n    accuracy = accuracy_score(ground_truth, predictions)\n    precision = precision_score(ground_truth, predictions, average='weighted')\n    recall = recall_score(ground_truth, predictions, average='weighted')\n    f1 = f1_score(ground_truth, predictions, average='weighted')\n\n    # BLEU Score\n    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n\n    results = {\n        'loss': total_loss / len(data_loader),\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'bleu_score': avg_bleu\n    }\n    \n    # Display results row by row\n    for metric, value in results.items():\n        print(f\"{metric.capitalize()}: {value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:41:14.999912Z","iopub.execute_input":"2024-11-26T10:41:15.000283Z","iopub.status.idle":"2024-11-26T10:41:15.009497Z","shell.execute_reply.started":"2024-11-26T10:41:15.000254Z","shell.execute_reply":"2024-11-26T10:41:15.008626Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"print(\"RNN Metrics:\\n\")\nrnn_metrics = evaluate_model(rnn_model, test_loader, criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:50:05.932272Z","iopub.execute_input":"2024-11-26T10:50:05.932618Z","iopub.status.idle":"2024-11-26T10:50:05.969992Z","shell.execute_reply.started":"2024-11-26T10:50:05.932582Z","shell.execute_reply":"2024-11-26T10:50:05.968871Z"}},"outputs":[{"name":"stdout","text":"RNN Metrics:\n\nLoss: 1.6429\nAccuracy: 0.2727\nPrecision: 0.0744\nRecall: 0.2727\nF1_score: 0.1169\nBleu_score: 0.2727\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"print(\"Bi-RNN Metrics:\\n\")\nbi_rnn_metrics = evaluate_model(bi_rnn_model, test_loader, criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:50:23.728059Z","iopub.execute_input":"2024-11-26T10:50:23.728392Z","iopub.status.idle":"2024-11-26T10:50:23.772867Z","shell.execute_reply.started":"2024-11-26T10:50:23.728362Z","shell.execute_reply":"2024-11-26T10:50:23.771780Z"}},"outputs":[{"name":"stdout","text":"Bi-RNN Metrics:\n\nLoss: 1.6324\nAccuracy: 0.0909\nPrecision: 0.0101\nRecall: 0.0909\nF1_score: 0.0182\nBleu_score: 0.0909\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"print(\"GRU Metrics:\\n\")\ngru_metrics = evaluate_model(gru_model, test_loader, criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:50:33.072637Z","iopub.execute_input":"2024-11-26T10:50:33.073326Z","iopub.status.idle":"2024-11-26T10:50:33.120425Z","shell.execute_reply.started":"2024-11-26T10:50:33.073297Z","shell.execute_reply":"2024-11-26T10:50:33.119383Z"}},"outputs":[{"name":"stdout","text":"GRU Metrics:\n\nLoss: 1.6078\nAccuracy: 0.2727\nPrecision: 0.0744\nRecall: 0.2727\nF1_score: 0.1169\nBleu_score: 0.2727\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"print(\"LSTM Metrics:\\n\")\nlstm_metrics = evaluate_model(lstm_model, test_loader, criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T10:50:37.826143Z","iopub.execute_input":"2024-11-26T10:50:37.826723Z","iopub.status.idle":"2024-11-26T10:50:37.853204Z","shell.execute_reply.started":"2024-11-26T10:50:37.826691Z","shell.execute_reply":"2024-11-26T10:50:37.852172Z"}},"outputs":[{"name":"stdout","text":"LSTM Metrics:\n\nLoss: 1.6178\nAccuracy: 0.2727\nPrecision: 0.0744\nRecall: 0.2727\nF1_score: 0.1169\nBleu_score: 0.2727\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":71},{"cell_type":"markdown","source":"## Part 2: Transformer (Text generation) ","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom transformers import TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:00:59.185038Z","iopub.execute_input":"2024-11-26T11:00:59.185707Z","iopub.status.idle":"2024-11-26T11:01:11.770511Z","shell.execute_reply.started":"2024-11-26T11:00:59.185669Z","shell.execute_reply":"2024-11-26T11:01:11.769538Z"}},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":"### 1. Fine tune the pre-trained model (GPT2)  ","metadata":{}},{"cell_type":"markdown","source":"#### 1.1. Load the Pre-trained GPT-2 Model   ","metadata":{}},{"cell_type":"code","source":"# Load pre-trained GPT-2 model and tokenizer\nmodel_name = \"gpt2\"\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\n\n# Set the model in evaluation mode\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:02:43.295885Z","iopub.execute_input":"2024-11-26T11:02:43.296716Z","iopub.status.idle":"2024-11-26T11:02:50.689079Z","shell.execute_reply.started":"2024-11-26T11:02:43.296679Z","shell.execute_reply":"2024-11-26T11:02:50.688093Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd4a963991ea4eaeb5da35878d20e5e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1405071bffc49ceb72a0919e4d7819a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b81dea9a84e4df8a71ab08dbf5953cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02de4767e59c4aa4bfee1068a0e66754"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11e0b5efe7914ae4a8ee814458a99acc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cf49dbc248d44ac990c75a70f69336a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e76a840ead9749a58310df8b28f1dffa"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":73},{"cell_type":"markdown","source":"#### 1.2. Load Shakespeare Dataset   ","metadata":{}},{"cell_type":"code","source":"# Load the dataset\ndataset_path = \"/kaggle/input/shakespeare-text/text.txt\"\n\n# Read the text dataset\nwith open(dataset_path, 'r') as file:\n    shakespeare_text = file.read()\n\n# Check the first 500 characters\nprint(shakespeare_text[:500])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:05:32.901487Z","iopub.execute_input":"2024-11-26T11:05:32.901953Z","iopub.status.idle":"2024-11-26T11:05:32.927917Z","shell.execute_reply.started":"2024-11-26T11:05:32.901918Z","shell.execute_reply":"2024-11-26T11:05:32.927067Z"}},"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor\n","output_type":"stream"}],"execution_count":74},{"cell_type":"markdown","source":"#### 1.3. Fine-tuning GPT-2 on the Shakespeare Dataset   ","metadata":{}},{"cell_type":"code","source":"# Prepare the dataset for fine-tuning\ntrain_file = \"/kaggle/working/shakespeare_train.txt\"\n\n# Write the dataset to a temporary file (required by TextDataset)\nwith open(train_file, \"w\") as file:\n    file.write(shakespeare_text)\n\n# Load the dataset using TextDataset\ntrain_dataset = TextDataset(\n    tokenizer=tokenizer,\n    file_path=train_file,\n    block_size=128  # Sequence length\n)\n\n# Set up data collator (used for padding and batching)\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False  # GPT-2 uses causal language modeling\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:11:14.882263Z","iopub.execute_input":"2024-11-26T11:11:14.883105Z","iopub.status.idle":"2024-11-26T11:11:14.908327Z","shell.execute_reply.started":"2024-11-26T11:11:14.883069Z","shell.execute_reply":"2024-11-26T11:11:14.907421Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./gpt2_finetuned\",  # Where to save the fine-tuned model\n    overwrite_output_dir=True,\n    num_train_epochs=5,  # Train for 3 epochs (adjustable)\n    per_device_train_batch_size=4,\n    save_steps=500,  # Save model every 500 steps\n    logging_steps=100,  # Log every 100 steps\n    report_to=[\"none\"]\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset\n)\n\n# Fine-tune the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:11:39.824801Z","iopub.execute_input":"2024-11-26T11:11:39.825156Z","iopub.status.idle":"2024-11-26T11:20:00.235996Z","shell.execute_reply.started":"2024-11-26T11:11:39.825126Z","shell.execute_reply":"2024-11-26T11:20:00.235192Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1650' max='1650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1650/1650 08:18, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>3.881300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>3.658600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>3.624300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>3.490700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>3.398900</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>3.414400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>3.357300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>3.269600</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>3.283300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.271900</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>3.199800</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>3.193600</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>3.209900</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>3.143900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>3.133800</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>3.153600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1650, training_loss=3.34943359375, metrics={'train_runtime': 499.9363, 'train_samples_per_second': 26.403, 'train_steps_per_second': 3.3, 'total_flos': 862263705600000.0, 'train_loss': 3.34943359375, 'epoch': 5.0})"},"metadata":{}}],"execution_count":77},{"cell_type":"markdown","source":"### 2. Generate Text Using the Fine-tuned Model  ","metadata":{}},{"cell_type":"code","source":"# Ensure the model and inputs are on the correct device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:24:25.758606Z","iopub.execute_input":"2024-11-26T11:24:25.759486Z","iopub.status.idle":"2024-11-26T11:24:25.770697Z","shell.execute_reply.started":"2024-11-26T11:24:25.759452Z","shell.execute_reply":"2024-11-26T11:24:25.769830Z"}},"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":81},{"cell_type":"code","source":"# Define an input sentence\ninput_sentence = \"As the sun set, the world seemed to quiet down\"\n\n# Encode the input sentence\ninput_ids = tokenizer.encode(input_sentence, return_tensors='pt').to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:31:40.453005Z","iopub.execute_input":"2024-11-26T11:31:40.453857Z","iopub.status.idle":"2024-11-26T11:31:40.458719Z","shell.execute_reply.started":"2024-11-26T11:31:40.453821Z","shell.execute_reply":"2024-11-26T11:31:40.457788Z"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":"# Generate new text (next tokens) using the fine-tuned model\ngenerated_ids = model.generate(\n    input_ids=input_ids,\n    max_length=200,  # Length of the generated text\n    num_return_sequences=1,  # Generate one sequence\n    temperature=0.9,  # Add randomness\n    top_k=50,  # Limit to the top 50 most likely next words\n    top_p=0.95,  # Use top-p sampling for diversity\n    no_repeat_ngram_size=2,  # Prevent repeating phrases\n    pad_token_id=tokenizer.eos_token_id  # Avoid issues with padding\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:31:42.057814Z","iopub.execute_input":"2024-11-26T11:31:42.058192Z","iopub.status.idle":"2024-11-26T11:31:43.948624Z","shell.execute_reply.started":"2024-11-26T11:31:42.058159Z","shell.execute_reply":"2024-11-26T11:31:43.947599Z"}},"outputs":[],"execution_count":103},{"cell_type":"code","source":"# Decode the generated ids to text\ngenerated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\n# Print the generated text\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T11:31:45.547258Z","iopub.execute_input":"2024-11-26T11:31:45.548055Z","iopub.status.idle":"2024-11-26T11:31:45.556118Z","shell.execute_reply.started":"2024-11-26T11:31:45.548019Z","shell.execute_reply":"2024-11-26T11:31:45.555069Z"}},"outputs":[{"name":"stdout","text":"As the sun set, the world seemed to quiet down,\nAnd the stars were still in their stars.\n\nBENVOLIO:\nO, I am a little too late!\nI am not a man to be late. I have been\nA little late, and am yet a very late;\nBut I will be so, for I must be a late\nTo be the late of the day. Come, come, my lord. What\nYou have done, you have made a mistake. You\nHave made an error, sir, in your haste. Your\nson, Angelo, is dead; and you, your son, are dead. Go, go, good sir; go. Away, away, home, Away! Away. Wherefore, what is your\ngoodly son?\nWhat is his name? Angelo? What is he? I'll tell you. Angelo! what\nis he, that is not Angelo: he is a poor,\n","output_type":"stream"}],"execution_count":104},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}